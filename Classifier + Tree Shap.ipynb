{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAULT_CODE_MAPPING = {\n",
    "    'F1L': 'Inverter Fault (Low)',\n",
    "    'F1M': 'Inverter Fault (Med)',\n",
    "    'F2L': 'Feedback Sensor Fault (Low)',\n",
    "    'F2M': 'Feedback Sensor Fault (Med)',\n",
    "    'F3L': 'Grid Anomaly (Low)',\n",
    "    'F3M': 'Grid Anomaly (Med)',\n",
    "    'F4L': 'PV Array Mismatch (Low)',\n",
    "    'F4M': 'PV Array Mismatch (Med)',\n",
    "    'F5L': 'PV Array Mismatch (Low)',\n",
    "    'F5M': 'PV Array Mismatch (Med)',\n",
    "    'F6L': 'MPPT Controller Fault (Low)',\n",
    "    'F6M': 'MPPT Controller Fault (Med)',\n",
    "    'F7L': 'Boost Converter Controller Fault (Low)',\n",
    "    'F7M': 'Boost Converter Controller Fault (Med)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_anomalies_only(input_path='vae_anomaly_test_results_1.csv', output_path='test1_anomalies.csv'):\n",
    "    try:\n",
    "        df = pd.read_csv(input_path)\n",
    "        anomalies = df[df['Anomaly'] == 1].copy()\n",
    "        if anomalies.empty:\n",
    "            print(f\"Warning: No anomalies found in {input_path}. Output file will be empty.\")\n",
    "        anomalies.to_csv(output_path, index=False)\n",
    "        print(f\"Filtered anomalies saved to: {output_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at {input_path}\")\n",
    "    except KeyError:\n",
    "        print(f\"Error: 'Anomaly' column not found in {input_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during filtering: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf96e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(train_path, test_path):\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        train_anomalies = train_df[train_df['Anomaly'] == 1].copy()\n",
    "        if train_anomalies.empty:\n",
    "            raise ValueError(f\"No anomalies found in the training file {train_path} for training the source classifier.\")\n",
    "\n",
    "        test_anomalies = pd.read_csv(test_path)\n",
    "        if test_anomalies.empty:\n",
    "            print(f\"Warning: The test anomalies file {test_path} is empty. No predictions or explanations will be generated.\")\n",
    "            return pd.DataFrame(), np.array([]), pd.DataFrame(), pd.DataFrame(), LabelEncoder()\n",
    "\n",
    "        feature_cols = ['Vpv', 'Vdc', 'ia', 'ib', 'ic', 'Vabc']  # Use physical features\n",
    "        if not all(col in train_anomalies.columns for col in feature_cols):\n",
    "            raise KeyError(f\"Training file is missing one or more required features: {feature_cols}\")\n",
    "        if 'source' not in train_anomalies.columns:\n",
    "            raise KeyError(\"Training file is missing the 'source' column.\")\n",
    "        if not all(col in test_anomalies.columns for col in feature_cols):\n",
    "            raise KeyError(f\"Test file is missing one or more required features: {feature_cols}\")\n",
    "\n",
    "        X_train = train_anomalies[feature_cols]\n",
    "        y_train = train_anomalies['source']\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "        X_test = test_anomalies[feature_cols]\n",
    "\n",
    "        print(f\"Data loaded. Training features shape: {X_train.shape}, Test features shape: {X_test.shape}\")\n",
    "        print(f\"Target classes found: {list(label_encoder.classes_)}\")\n",
    "\n",
    "        return X_train, y_train_encoded, X_test, test_anomalies, label_encoder\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Data file not found: {e}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Column {e} not found in one of the CSV files.\")\n",
    "        raise\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in data value: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data loading: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd12c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    if X_train.empty or len(y_train) == 0:\n",
    "        print(\"Error: Cannot train model with empty data.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Training XGBoost classifier...\")\n",
    "    model = xgb.XGBClassifier(objective='multi:softprob',\n",
    "                              eval_metric='mlogloss',\n",
    "                              random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    model_filename = 'xgb_classifier_model.json'\n",
    "    model.save_model(model_filename)\n",
    "    print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a41390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sources(model, X_test, label_encoder):\n",
    "    if model is None or X_test.empty:\n",
    "        print(\"Skipping prediction due to missing model or empty test data.\")\n",
    "        return None, None\n",
    "\n",
    "    print(\"Making predictions on test anomalies...\")\n",
    "    y_pred_encoded = model.predict(X_test)\n",
    "    predicted_sources = label_encoder.inverse_transform(y_pred_encoded)\n",
    "    print(\"Prediction complete.\")\n",
    "    return y_pred_encoded, predicted_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee56fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_predictions_with_shap(model, X_test, test_df_with_predictions, label_encoder):\n",
    "    if model is None or X_test.empty:\n",
    "        print(\"Skipping SHAP explanations due to missing model or empty test data.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Generating SHAP Explanations ---\")\n",
    "\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        print(\"Calculating SHAP values (this may take a moment)...\")\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        print(\"SHAP values calculated.\")\n",
    "\n",
    "        expected_value = explainer.expected_value\n",
    "\n",
    "        num_explanations_to_show = min(5, len(X_test))\n",
    "        shap_details_list = []  # To save SHAP values to CSV\n",
    "\n",
    "        if 'Predicted_Source' not in test_df_with_predictions.columns:\n",
    "            print(\"Error: 'Predicted_Source' column not found in the prediction DataFrame. Cannot generate force plots or explanations.\")\n",
    "            return\n",
    "\n",
    "        y_pred_encoded = label_encoder.transform(test_df_with_predictions['Predicted_Source'])\n",
    "\n",
    "        for i in range(len(X_test)):\n",
    "            instance_idx = i\n",
    "            predicted_class_idx = y_pred_encoded[instance_idx]\n",
    "            predicted_class_name = label_encoder.classes_[predicted_class_idx]\n",
    "            predicted_fault_description = FAULT_CODE_MAPPING.get(predicted_class_name, 'Unknown Fault')\n",
    "\n",
    "            print(f\"\\nExplaining Anomaly {instance_idx} (Predicted Source: {predicted_class_name})\")\n",
    "\n",
    "            try:\n",
    "                if isinstance(shap_values, list):\n",
    "                    class_shap_values = shap_values[predicted_class_idx][instance_idx, :]\n",
    "                    base_value = expected_value[predicted_class_idx] if isinstance(expected_value, (list, np.ndarray)) else expected_value\n",
    "                else:\n",
    "                    class_shap_values = shap_values[instance_idx, :, predicted_class_idx] if shap_values.ndim == 3 else shap_values[instance_idx, :]\n",
    "                    base_value = expected_value[predicted_class_idx] if isinstance(expected_value, (list, np.ndarray)) else expected_value\n",
    "\n",
    "                # Save SHAP explanation details into list\n",
    "                feature_contributions = dict(zip(X_test.columns, class_shap_values))\n",
    "                sorted_features = sorted(feature_contributions.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "                top_features = {f\"TopFeature_{rank+1}\": feature for rank, (feature, _) in enumerate(sorted_features[:3])}\n",
    "                top_contributions = {f\"TopContribution_{rank+1}\": contrib for rank, (_, contrib) in enumerate(sorted_features[:3])}\n",
    "\n",
    "                shap_details = {\n",
    "                    'Anomaly_Index': instance_idx,\n",
    "                    'Predicted_Source_Code': predicted_class_name,\n",
    "                    'Predicted_Fault_Description': predicted_fault_description,\n",
    "                    **top_features,\n",
    "                    **top_contributions\n",
    "                }\n",
    "                shap_details_list.append(shap_details)\n",
    "\n",
    "                # Plot and save the SHAP force plot for the current anomaly\n",
    "                '''shap.initjs()\n",
    "                force_plot = shap.force_plot(base_value, class_shap_values, X_test.iloc[instance_idx, :])\n",
    "                force_plot_path = f\"shap_force_plot_anomaly_{instance_idx}.html\"\n",
    "                shap.save_html(force_plot_path, force_plot)\n",
    "                print(f\"Force plot saved for anomaly {instance_idx} to {force_plot_path}\")'''\n",
    "\n",
    "            except Exception as plot_error:\n",
    "                print(f\"Error explaining instance {instance_idx}: {plot_error}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "        # After looping, save all shap explanations to CSV\n",
    "        shap_df = pd.DataFrame(shap_details_list)\n",
    "        shap_df.to_csv('shap_explanations_per_anomaly.csv', index=False)\n",
    "        print(f\"\\nSaved detailed SHAP explanations to shap_explanations_per_anomaly.csv\")\n",
    "\n",
    "        print(\"\\n--- SHAP Explanations Complete ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An unexpected error occurred during SHAP explanation generation ---\")\n",
    "        traceback.print_exc()\n",
    "        print(f\"Error message: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filter_anomalies_only()\n",
    "    \n",
    "    train_path = 'vae_anomaly_test_results.csv'\n",
    "    test_path = 'test1_anomalies.csv'\n",
    "    \n",
    "    try:\n",
    "        X_train, y_train_encoded, X_test, test_anomalies, label_encoder = load_and_prepare_data(train_path, test_path)\n",
    "        \n",
    "        if X_train.empty or X_test.empty:\n",
    "            print(\"Cannot proceed with empty datasets.\")\n",
    "            return\n",
    "            \n",
    "        model = train_classifier(X_train, y_train_encoded)\n",
    "        \n",
    "        y_pred_encoded, predicted_sources = predict_sources(model, X_test, label_encoder)\n",
    "        \n",
    "        if predicted_sources is not None:\n",
    "            test_with_predictions = test_anomalies.copy()\n",
    "            test_with_predictions['Predicted_Source'] = predicted_sources\n",
    "            test_with_predictions['Predicted_Source_Code'] = label_encoder.transform(predicted_sources)\n",
    "            explain_predictions_with_shap(model, X_test, test_with_predictions, label_encoder)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during the pipeline execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
